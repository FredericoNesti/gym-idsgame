* V16 Minimal defense PPO
Randomize Env: Yes
Fully observed: Yes
Randomize starting position: Yes
#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.00001, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=100, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=1,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=100000, attacker=True, defender=False,
                                                video_frequency=101,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=5000,
                                                input_dim_attacker=(4 + 2) * 3,
                                                output_dim_attacker=4 * 3,
                                                input_dim_defender=(4 + 2) * 3,
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=4, batch_size=2000,
                                                gpu=False, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=1, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False)
#+end_src
* V18 Minimal defense PPO

Randomize Env: Yes
Fully observed: No
Randomize starting position: Yes
Reconnaissance bool features: Yes
reconnaissance reward: No
#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.00001, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=100, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=1,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                video_frequency=101,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=5000,
                                                input_dim_attacker=((4 + 2) * 3),
                                                output_dim_attacker=(4+1)* 3,
                                                input_dim_defender=((4 + 1) * 3),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=32,
                                                num_hidden_layers=2, batch_size=2000,
                                                gpu=False, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.1, max_gradient_norm=0.5, gae_lambda=0.99,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=1, ent_coef=0.1)
#+end_src





Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward: No

#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.000008, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=100, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=1,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                video_frequency=101,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=5000,
                                                input_dim_attacker=((4 + 2) * 2),
                                                output_dim_attacker=(4+1)* 2,
                                                input_dim_defender=((4 + 1) * 3),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=4, batch_size=2000,
                                                gpu=False, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.05,
                                                render_attacker_view=True)
#+end_src



Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward: No

#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.000008, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=100, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=1,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                video_frequency=101,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=5000,
                                                input_dim_attacker=((4 + 2) * 2),
                                                output_dim_attacker=(4+1)* 2,
                                                input_dim_defender=((4 + 1) * 3),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=4, batch_size=2000,
                                                gpu=True, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.05,
                                                render_attacker_view=True)
#+end_src

Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward: No

#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.000008, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=100, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=1,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                Video_frequency=101,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=250,
                                                input_dim_attacker=((4 + 2) * 2),
                                                output_dim_attacker=(4+1)* 2,
                                                input_dim_defender=((4 + 1) * 3),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=4, batch_size=2000,
                                                gpu=False, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.01,
                                                render_attacker_view=True)
#+end_src
Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward: No
#+begin_src python
    pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.00003, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=1000, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=500,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                video_frequency=1001,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=250,
                                                input_dim_attacker=((4 + 2) * 2),
                                                output_dim_attacker=(4 + 1) * 2,
                                                input_dim_defender=((4 + 1) * 3),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=32,
                                                num_hidden_layers=2, batch_size=2000,
                                                gpu=True, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.00,
                                                render_attacker_view=True, lr_progress_power_decay=4,
                                                lr_progress_decay=True, use_sde=False, sde_sample_freq=4,
                                                one_hot_obs=False)
    # input_dim_attacker = (3, 3, 5),
    # output_dim_attacker = (5 * 2) * 3,
    # input_dim_defender = (3, 3, 5),
    # output_dim_defender = 6 * 3,
    # input_dim_attacker = ((5 * 2 + 1) * 3),
    # output_dim_attacker = (5 * 2) * 3,
    # input_dim_defender = ((5 + 1) * 3),
    # output_dim_defender = 6 * 3,
    env_name = "idsgame-minimal_defense-v18"
    client_config = ClientConfig(env_name=env_name, attacker_type=AgentType.PPO_OPENAI_AGENT.value,
                                 mode=RunnerMode.TRAIN_ATTACKER.value,
                                 pg_agent_config=pg_agent_config, output_dir=default_output_dir(),
                                 title="OpenAI-PPO vs DefendMinimalDefender",
                                 run_many=False, random_seeds=[0, 999, 299, 399, 499])
    # client_config = hp_tuning_config(client_config)
    return client_config
#+end_src

Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward: No
#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.00003, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=1000, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=500,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                video_frequency=1001,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=250,
                                                input_dim_attacker=((4 + 2) * 2),
                                                output_dim_attacker=(4 + 1) * 2,
                                                input_dim_defender=((4 + 1) * 3),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=4, batch_size=2000,
                                                gpu=True, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.005,
                                                render_attacker_view=True, lr_progress_power_decay=4,
                                                lr_progress_decay=True, use_sde=False, sde_sample_freq=4,
                                                one_hot_obs=False)
#+end_src
* V19 Minimal defense PPO

Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward: No
#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.00003, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=1000, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=500,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                video_frequency=1001,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=250,
                                                input_dim_attacker=((4 + 2) * 4),
                                                output_dim_attacker=(4 + 1) * 4,
                                                input_dim_defender=((4 + 1) * 4),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=2, batch_size=2000,
                                                gpu=False, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.00,
                                                render_attacker_view=True, lr_progress_power_decay=4,
                                                lr_progress_decay=True, use_sde=False, sde_sample_freq=4,
                                                one_hot_obs=False)
#+end_src
Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward: No
#+begin_src python
    pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.00003, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=1000, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=500,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                video_frequency=1001,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=250,
                                                input_dim_attacker=((4 + 2) * 4),
                                                output_dim_attacker=(4 + 1) * 4,
                                                input_dim_defender=((4 + 1) * 4),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=2, batch_size=2000,
                                                gpu=False, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.00,
                                                render_attacker_view=True, lr_progress_power_decay=4,
                                                lr_progress_decay=True, use_sde=False, sde_sample_freq=4,
                                                one_hot_obs=False)
#+end_src
Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward: No
#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.0001, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=1000, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=500,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=175000, attacker=True, defender=False,
                                                video_frequency=1001,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=250,
                                                input_dim_attacker=((4 + 2) * 4),
                                                output_dim_attacker=(4 + 1) * 4,
                                                input_dim_defender=((4 + 1) * 4),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=32,
                                                num_hidden_layers=2, batch_size=2000,
                                                gpu=False, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.00,
                                                render_attacker_view=True, lr_progress_power_decay=4,
                                                lr_progress_decay=True, use_sde=False, sde_sample_freq=4,
                                                one_hot_obs=False, force_exploration=True, force_exp_p=0.25)
#+end_src
Randomize Env: Yes
Fully Observed: No
Randomize Starting position Yes
Local view: Yes
Randomize rec states: yes
reconnaissance bool features: yes
reconnaissance reward:Yes
#+begin_src python
pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.0001, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=1000, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=500,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=75000, attacker=True, defender=False,
                                                video_frequency=1001,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=75,
                                                input_dim_attacker=((4 + 2) * 4),
                                                output_dim_attacker=(4 + 1) * 4,
                                                input_dim_defender=((4 + 1) * 4),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=2, batch_size=2000,
                                                gpu=False, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=0, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.001,
                                                render_attacker_view=True, lr_progress_power_decay=4,
                                                lr_progress_decay=True, use_sde=False, sde_sample_freq=4,
                                                one_hot_obs=False, force_exploration=False, force_exp_p=0.2)
#+end_src
#+begin_src python
    pg_agent_config = PolicyGradientAgentConfig(gamma=1, alpha_attacker=0.0001, epsilon=1, render=False,
                                                alpha_defender=0.0001,
                                                eval_sleep=0.9,
                                                min_epsilon=0.01, eval_episodes=3, train_log_frequency=1,
                                                epsilon_decay=0.9999, video=True, eval_log_frequency=500,
                                                video_fps=5, video_dir=default_output_dir() + "/results/videos",
                                                num_episodes=100000000,
                                                eval_render=False, gifs=True,
                                                gif_dir=default_output_dir() + "/results/gifs",
                                                eval_frequency=25000, attacker=True, defender=False,
                                                video_frequency=1,
                                                save_dir=default_output_dir() + "/results/data",
                                                checkpoint_freq=75,
                                                input_dim_attacker=((4 + 2) * 4),
                                                output_dim_attacker=(4 + 1) * 4,
                                                input_dim_defender=((4 + 1) * 4),
                                                output_dim_defender=5 * 3,
                                                hidden_dim=64,
                                                num_hidden_layers=4, batch_size=2000,
                                                gpu=True, tensorboard=True,
                                                tensorboard_dir=default_output_dir() + "/results/tensorboard",
                                                optimizer="Adam", lr_exp_decay=False, lr_decay_rate=0.999,
                                                state_length=1, normalize_features=False, merged_ad_features=True,
                                                zero_mean_features=False, gpu_id=1, lstm_network=False,
                                                lstm_seq_length=4, num_lstm_layers=2, optimization_iterations=10,
                                                eps_clip=0.2, max_gradient_norm=0.5, gae_lambda=0.95,
                                                cnn_feature_extractor=False, features_dim=512,
                                                flatten_feature_planes=False, cnn_type=5, vf_coef=0.5, ent_coef=0.0,
                                                render_attacker_view=True, lr_progress_power_decay=4,
                                                lr_progress_decay=True, use_sde=False, sde_sample_freq=4,
                                                one_hot_obs=False, force_exploration=False, force_exp_p=0.2)
#+end_src
